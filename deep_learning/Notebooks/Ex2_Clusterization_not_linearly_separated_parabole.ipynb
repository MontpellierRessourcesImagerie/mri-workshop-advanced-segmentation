{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Ex2_Clusterization_not_linearly_separated_parabole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Hq9m_Kdl1M"
      },
      "source": [
        "# Step II : Non-linear classification\n",
        "\n",
        "In this example, similar to the previous one, the set of coordinates of the xy points are now separated using a parabolic curve. This kind of problem cannot be solved with a single neuron (which separates the point always by a straight line) and a deeper network will be needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWMAcdXpdl1O"
      },
      "source": [
        "Here we define the training set, formed by $XY$ points belonging to two classes, defined by \n",
        "\n",
        "$Y<aX²+bX+c$ \n",
        "\n",
        "or \n",
        "\n",
        "$Y>aX²+bX+c$. \n",
        "\n",
        "The Validation data set and a Testing data set are also defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2kHba3Tdl1Q"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "    \n",
        "def Training_set_parabole(a,b,c,Ntrain,Nval,Ntest):\n",
        "    \n",
        "    n = 0\n",
        "    N = Ntrain+Nval+Ntest\n",
        "    Data = np.zeros([N,2])\n",
        "    Labels = np.zeros([N,1])\n",
        "      \n",
        "    for n in range(0,N):\n",
        "        x = random.uniform(-1, 1)\n",
        "        y = random.uniform(-1, 1)\n",
        "        Data[n,0] = x\n",
        "        Data[n,1] = y\n",
        "        \n",
        "        if y < a*x*x+b*x+c :\n",
        "            Labels[n,0] = 1\n",
        "        else : \n",
        "            Labels[n,0] = 0\n",
        "\n",
        "    Training_data = Data[:Ntrain,]\n",
        "    Training_label = Labels[:Ntrain,]\n",
        "    Validation_data = Data[Ntrain+1:Ntrain+Nval,]\n",
        "    Validation_label = Labels[Ntrain+1:Ntrain+Nval,]\n",
        "    Testing_data = Data[Ntrain+Nval+1:N,]\n",
        "    Testing_label = Labels[Ntrain+Nval+1:N,]\n",
        "        \n",
        "    return Training_data, Testing_data, Validation_data, Training_label, Validation_label, Testing_label "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsSigCJedl1Z"
      },
      "source": [
        "The training/validation/testing sets are defined below. Try changing `Ntrain` (ex 20,..,1000), and see the effects on each model architecture you choose below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVKvfrbkdl1a"
      },
      "source": [
        "a = 2;\n",
        "b = 0;\n",
        "c = -1;\n",
        "Ntrain = 20\n",
        "Nval = 400\n",
        "Ntest = 400\n",
        "    \n",
        "Training_data, Testing_data, Validation_data, Training_label,Validation_label, Testing_label = Training_set_parabole(a,b,c,Ntrain,Nval,Ntest) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt9dTV2ErNPk"
      },
      "source": [
        "The training set is then plot using two different colors\n",
        "to distinguish the two classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJK1WCRQrJKt"
      },
      "source": [
        "Idx_class_0 = Training_label==0\n",
        "Idx_class_1 = Training_label==1\n",
        "\n",
        "X0 = Training_data[Idx_class_0[:,0],0]\n",
        "Y0 = Training_data[Idx_class_0[:,0],1]\n",
        "X1 = Training_data[Idx_class_1[:,0],0]\n",
        "Y1 = Training_data[Idx_class_1[:,0],1]\n",
        "\n",
        "X = np.linspace(-1, 1, 50)\n",
        "Y = a*X**2 + b*X + c  \n",
        "        \n",
        "plt.plot(X0, Y0, 'r.', ms=10)\n",
        "plt.plot(X1, Y1, 'b.', ms=10)\n",
        "plt.plot(X,Y,'--k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLxqFPURdl1f"
      },
      "source": [
        "Define the architecture of the model. Here we will start with a single neuron (as in the previous example) and then build a more complex network. The input are however the same : the X and Y coordinates. \n",
        "\n",
        "Since we are working with two classes, the last layer (if you have multiple layers) should have only a single neuron, and a \"sigmoid\" activation function, to give the model an output that is a single numnber in [0,1] which we interpret as the probability to be in one class. The loss function should be \"binary cross-entropy\", typical for classifications with 2 classes. In the end the distinction between the two classes will be made on the base of whether the output will be below or above 0,5.\n",
        "\n",
        "Note that the function \"model.summary\" will return a complete description of your network, highlighting the number of trainable parameters.\n",
        "\n",
        "Try different variations of the model: \n",
        "- 1 layer 1 neuron\n",
        "- 1 layer multiple neurons\n",
        "- multiple layers multiple neurons\n",
        " \n",
        "Find the differences among these options, with this data set that cannot be linearly separated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOG-VmWIdl1g"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# 1 layer, 1neuron - try this uncommenting only it. Choose the activation function:\n",
        "#model.add(layers.Dense( 1 , activation='???', input_shape=(2,)))\n",
        "\n",
        "# 1 layer, many neurons - try this uncommenting only it. Change the number of neurons, and choose the activation functions (relu or sigmoid?):\n",
        "#model.add(layers.Dense( ??? , activation='???', input_shape=(2,)))\n",
        "#model.add(layers.Dense( ???, activation='???'))\n",
        "\n",
        "# many layers, many neurons - try this uncommenting only it, and modify the neurons in each per layer, and/or the number of layers:\n",
        "#model.add(layers.Dense( ???, activation='???', input_shape=(2,)))\n",
        "#model.add(layers.Dense( ???, activation='???'))\n",
        "# ...\n",
        "#model.add(layers.Dense( ???, activation='???'))\n",
        "\n",
        "model.compile(optimizer = 'adam', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqQFnxDfdl1l"
      },
      "source": [
        "Training of the model. The number of Epoch and the batch size are defined below. The results at each iteraction are saved in order to compare the accuracy calculated for the training set and for the validation set. These data are\n",
        "saved in the variable history.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F935t0w2dl1m"
      },
      "source": [
        "history = model.fit(Training_data,\n",
        "                    Training_label,\n",
        "                    epochs = 200,\n",
        "                    batch_size = 16,\n",
        "                    validation_data = (Validation_data, Validation_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPElb1ERdl1q"
      },
      "source": [
        "The accuracy of the model is tested using the testing set of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rPyjXfPdl1r"
      },
      "source": [
        "Results = model.predict(Testing_data)\n",
        "\n",
        "Idx_class_0 = Results<0.5\n",
        "Idx_class_1 = Results>=0.5\n",
        "\n",
        "X0 = Testing_data[Idx_class_0[:,0],0]\n",
        "Y0 = Testing_data[Idx_class_0[:,0],1]\n",
        "X1 = Testing_data[Idx_class_1[:,0],0]\n",
        "Y1 = Testing_data[Idx_class_1[:,0],1]\n",
        "\n",
        "X = np.linspace(-1, 1, 50)\n",
        "Y = a*X**2 + b*X + c      \n",
        "        \n",
        "plt.plot(X0, Y0, 'r.')\n",
        "plt.plot(X1, Y1, 'b.')\n",
        "plt.plot(X,Y,'--k')\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JTYHL7hdl1w"
      },
      "source": [
        "In the same way, using the model.evaluate function you can test the accuracy of the model when working on the testing\n",
        "set. The second number returns the average accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcwj-_p8dl1w"
      },
      "source": [
        "Predication_accuracy = model.evaluate(Testing_data, Testing_label)\n",
        "print(Predication_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AuPQVEWdl1z"
      },
      "source": [
        "Below the accuracy for the training and validation sets are plotted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISYvLJ3dl10"
      },
      "source": [
        "history_dict = history.history\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "n = len(acc_values)\n",
        "epochs = range(1, n+1)\n",
        "\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiiy3hWXdl14"
      },
      "source": [
        "Below the validation loss for the training and validation sets are plotted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ly_39ddl15"
      },
      "source": [
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd2LM5tndl18"
      },
      "source": [
        "Let's plot the prediction of the testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLd7FF7odl18"
      },
      "source": [
        "Results = model.predict(Testing_data)\n",
        "\n",
        "X0 = []\n",
        "X1 = []\n",
        "Y0 = []\n",
        "Y1 = []\n",
        "#for n in range(0,Length):\n",
        "for n in range(0, len(Testing_data)):\n",
        "\n",
        "    if Results[n]<=0.5:\n",
        "        X0.append(Results[n])\n",
        "        Y0.append(Testing_data[n,0])\n",
        "    else:\n",
        "        X1.append(Results[n])\n",
        "        Y1.append(Testing_data[n,0])\n",
        "        \n",
        "plt.plot(Y0, X0, 'r.')\n",
        "plt.plot(Y1, X1, 'b.')\n",
        "plt.ylabel('Model output')\n",
        "plt.xlabel('X position')\n",
        "plt.axis('square')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx5XdvY_5BQI"
      },
      "source": [
        "Let's plot the prediction of a more regular testing set, which clearly shows the performance of the model with nw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impTdeBRdl2A"
      },
      "source": [
        "Idx_class_0 = Training_label==0\n",
        "Idx_class_1 = Training_label==1\n",
        "\n",
        "X0 = Training_data[Idx_class_0[:,0],0]\n",
        "Y0 = Training_data[Idx_class_0[:,0],1]\n",
        "X1 = Training_data[Idx_class_1[:,0],0]\n",
        "Y1 = Training_data[Idx_class_1[:,0],1]\n",
        "\n",
        "X = np.linspace(-1, 1, 50)\n",
        "Y = a*X**2 + b*X + c  \n",
        "        \n",
        "plt.plot(X0, Y0, 'r.', ms=10)\n",
        "plt.plot(X1, Y1, 'b.', ms=10)\n",
        "plt.plot(X,Y,'--k')\n",
        "plt.title('Training set')\n",
        "plt.show()\n",
        "\n",
        "x = np.linspace(-1, 1, 50)\n",
        "y = np.linspace(-1, 1, 50)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "X = X.flatten()\n",
        "Y = Y.flatten()\n",
        "\n",
        "Test = np.vstack((X,Y))\n",
        "Test = np.transpose(Test)\n",
        "Results = model.predict(Test)\n",
        "\n",
        "Idx_class_0 = Results<0.5\n",
        "Idx_class_1 = Results>=0.5\n",
        "\n",
        "X0 = Test[Idx_class_0[:,0],0]\n",
        "Y0 = Test[Idx_class_0[:,0],1]\n",
        "X1 = Test[Idx_class_1[:,0],0]\n",
        "Y1 = Test[Idx_class_1[:,0],1]\n",
        "\n",
        "X = np.linspace(-1, 1, 50)\n",
        "Y = a*X**2 + b*X + c        \n",
        "        \n",
        "plt.plot(X0, Y0, 'r.')\n",
        "plt.plot(X1, Y1, 'b.')\n",
        "plt.plot(X,Y,'--k')\n",
        "plt.title('Predictions')\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}