{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Identify the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image set contains two phenotypes. The cells so called positives that generate \"vesicle-type\" spots and the cells so called negatives that are normal.\n",
    "\n",
    "![](./exercises/svm/images/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the labels in the data are discrete, the predication falls into two categories, (i.e. Postive Cell or Negative Cell). In machine learning this is a classification problem.    \n",
    "> Thus, the goal is to classify whether cell  is positive or negative and predict the accuracy of the model with different classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Identify data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used image set [BBBC016v1](https://data.broadinstitute.org/bbbc/BBBC016/)[<sup>1</sup>](#fn1).   \n",
    "We used a part of this dataset taking account the wells O06, O07, O16 and O22.    \n",
    "Features were generated by CellProfiler and classes were annotated manually. The dataset contains **40 samples of positives and negatives cells**.\n",
    "* The first two columns in the dataset contain the *labels* (Positives, Negatives), and the *dose* put for each well.\n",
    "* The columns 4 - 5 contain the well position on the plate, the unique ID of the image and the number of object respectively.\n",
    "* The columns 6 - 155 contain *features* that have been computed from images of the cell nuclei and cell cytoplasm which can be used to build a model to predict the phenotype of the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Importing libraries.\n",
    "import numpy as np\n",
    "\n",
    "# b) Replace the occurences of ... to import the pandas library with the clause import. \n",
    "... ... as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Importing the dataset.\n",
    "\n",
    "# Replace the occurences of ... to indicate a string path \n",
    "# to your file (ie dataset.csv is into ML/exercises/svm/features). \n",
    "file = ...\n",
    "\n",
    "# Replace the occurences of ... to load the dataset.csv file (path assigned previously) \n",
    "# using the Pandas read_csv function. \n",
    "dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to visually inspect the dataset. There are multiple ways to achieve this:\n",
    "* The easiest being to request the first few records using the data.head() method. By default, “data.head()” returns the first 5 rows.\n",
    "* Alternatively, one can also use “data.tail()” to return the five last rows of the data.\n",
    "* For both head and tail methods, there is an option to specify the number of rows by including the required number in between the parentheses when calling either method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) print the dataset.\n",
    "# After reading the notes above try to display the ten rows of the dataset. Fill in the occurences of ...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the number of lines and columns using the shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Replace the occurence ... by the shape method to get the number of rows and number of columns\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result displayed, you should be have 40 records with 155 columns.   \n",
    "The “info()” method provides a concise summary of the data; It provides the type of data in each column, the number of non-null values in each column, and how much memory the data frame is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) Replace the occurence ... by the \"info(verbose = True)\" to get the data type of each column\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above Label, Dose and Well are *categorical variables* and the others are floating or integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is a crucial step for any data analysis problem. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use. This involves a number of activities such as:\n",
    "* Assigning numerical values to categorical data because the svm classifier works only with numerical values.\n",
    "* Handling missing values.\n",
    "* Divide data into attributes and labels sets.\n",
    "* Divide data into traininig and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal here is to encode the class Label in a list y and get attributes in an array X. Then split the data into a *training set* and *test set*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 split features and labels into new sets and encoding the labels into integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) To select a column from dataset use data['feature']. Replace the occurence ... to select the Label feature\n",
    "y = ...\n",
    "\n",
    "#transform the class labels from their original string representation (positive and negative) into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# b) Use the method drop(columns=['feature_1', \"feature_2\", ...]) \n",
    "# to drop unnecessary features 'Label', 'Dose', 'Well', 'ImageNumber', 'ObjectNumber'\n",
    "# and affect the output to X variable\n",
    "... = ...\n",
    "\n",
    "# c) Replace the occurence ... to display 5 rows of the X dataset with head() method\n",
    "print(..., y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After encoding the Label, the phenotype cell are now represented as class 1(i.e positive cell) and as class 0 (i.e negative cell), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest method to evaluate the performance of a machine learning algorithm is to use different training and test datasets. Here \n",
    "* The *train_test_split* method from the model.selection of scikit-learn split the available data into a training set and a testing set. (70% training, 30% test).\n",
    "* This method takes three parameters train_test_split: The first parameter will be the X dataset, the second parameter will be the y dataset and the third parameter will be the size of the testing set ie (70% = 0.70)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)Follow the instructions above to fill in the occurences ...\n",
    "\n",
    "from sklearn.model_selection import ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(..., ..., test_size = ...)\n",
    "\n",
    "# b) Fill in the occurences ... to display the number of rows \n",
    "# and numbers of columns for the two variables (X_train and X_test)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predictive model using Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal is to fit a linear model to the data using *SVC library* from the svm of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the instructions and fill in the occurences ...\n",
    "\n",
    "# a) Create an SVM classifier and train it on 70% of the data set.\n",
    "# use the support vector classifier class, which is written as SVC in the Scikit-Learn's svm library. \n",
    "# This class takes one parameter, which is the kernel type 'linear'.\n",
    "# We will see non-linear kernels in the next section.\n",
    "\n",
    "from sklearn.svm import ...\n",
    "svclassifier = ...(kernel= ... )\n",
    "\n",
    "# b) The fit method of SVC class is called to train the algorithm on the training data (X_train, y_train).\n",
    "# The training data is passed as a parameter to the fit method.\n",
    "\n",
    "...(... , ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result you will see the important parameters in kernel SVMs:\n",
    "* Regularization parameter C.\n",
    "\n",
    "* The choice of the kernel (linear, radial basis function(RBF) or polynomial).\n",
    "\n",
    "* Kernel-specific parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, the predict method of the SVC class is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Fill in the occurences to make prediction on the test set (X_test) \n",
    "y_pred = ...(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Confusion matrix* is the most commonly used metric for classification tasks. Scikit-Learn's metrics library contains the confusion_matrix method, which can be readily used to find out the values for these important metrics.   \n",
    "The confusion matrix that essentially is a *two-dimensional table* where the classifier model is on one axis (vertical), and ground truth is on the other (horizontal) axis, as shown below. Either of these axes can take two values (as depicted)   \n",
    "\n",
    "Model says \"+\" | Model says \"-\" | Ground truth\n",
    "---------------|----------------|-----------\n",
    "`True positive`|`False negative`|Actual: \"+\"\n",
    "`False positive`|`True negative`|  Actual: \"-\"\n",
    "\n",
    "> The goal is to create a confusion matrix in order to know the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) fill in the occurences to import confusion_matrix method from sklearn.metrics\n",
    "\n",
    "from ... import ...  \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "There are two possible predicted classes: \"1\" (i.e positive cell) and \"0\" (i.e negative cell).\n",
    "\n",
    "    a) How many cells are predicted trully positives ?   \n",
    "    b) How many cells are predicted trully negatives ?    \n",
    "    c) How many cells are predicted falsly positives ?     \n",
    "    d) How many cells are predicted falsly negatives ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Rates as computed from the confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) **Accuracy**: Overall, how often is the classifier correct? Calculate the accuracy of the model in percentage   \n",
    "     $$Accuracy = (\\frac{TP+TN}{TP+TN+FP+FN})*100$$   \n",
    "     Accuracy = ...%\n",
    "     \n",
    "b) **Misclassification Rate**: Overall, how often is it wrong? Calculate the \"error rate\" of the model in percentage\n",
    "    $$Error Rate = (\\frac{FP+FN}{TP+TN+FP+FN})\\cdot100$$   \n",
    "    Error Rate = ...%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Comparison between different kernel for non linear classification\n",
    "\n",
    "> We will use polynomial, Gaussian, and sigmoid kernels to see which one works better for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Replace the ... kernel parameter from the SVC class by 'poly'\n",
    "svclassifier = SVC(kernel='...')  \n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Make predictions on the test set\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) create a confusion matrix to evaluate the accuracy\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) calculate the accuracy and the error rate of the polynomial model.    \n",
    "Accuracy = ...%     \n",
    "Eror rate = ... %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Which one kernel works best ?\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Make prediction on new dataset (unlabel)\n",
    "\n",
    "> the goal is to predict unlabel dataset with the best classifier run above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclassifier = SVC(kernel='...')  \n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) importing the new dataset (file: unlabel_dataset.csv is into /exercises/svm/features)\n",
    "new_data = ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) print the 8 rows of new_data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) drop unrelevant features like 'Dose', 'Well', 'ImageNumber', 'ObjectNumber'\n",
    "x_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) prediction on the new_data\n",
    "x_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = new_data.assign(Prediction = x_pred)\n",
    "print(labelled_data[['Dose', 'Well', 'ImageNumber', 'ObjectNumber', 'Prediction']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnote\n",
    "\n",
    "<span id=\"fn1\"> We used images set provided by Ilya Ravkin, available from the Broad Bioimage Benchmark Collection [Ljosa et al., Nature Methods, 2012]</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
